{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar RUT de Nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def separate_profesor_data():\n",
    "    # Read the CSV file with semicolon delimiter\n",
    "    df = pd.read_csv('Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns for RUT and Professor name\n",
    "    df['RUT'] = ''\n",
    "    df['Profesor_Nombre'] = ''\n",
    "    \n",
    "    # Process only rows where 'Profesor' column is not empty and contains a pattern matching a RUT\n",
    "    mask = df['Profesor'].notna() & df['Profesor'].str.contains(r'\\d+-', na=False)\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        profesor_str = df.at[idx, 'Profesor']\n",
    "        # Split by first space after the RUT pattern\n",
    "        parts = profesor_str.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            df.at[idx, 'RUT'] = parts[0]\n",
    "            df.at[idx, 'Profesor_Nombre'] = parts[1]\n",
    "    \n",
    "    # Drop the original 'Profesor' column\n",
    "    df = df.drop('Profesor', axis=1)\n",
    "    \n",
    "    # Reorder columns to put RUT and Profesor_Nombre after the fifth column\n",
    "    cols = df.columns.tolist()\n",
    "    # Ensure RUT and Profesor_Nombre are only inserted once\n",
    "    cols = cols[:5] + ['RUT', 'Profesor_Nombre'] + [col for col in cols[5:] if col not in ['RUT', 'Profesor_Nombre']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN1_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = separate_profesor_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar Capacidad de Inscritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def capacity_and_enrolled():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN1_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns\n",
    "    df['Vacantes'] = ''\n",
    "    df['Inscritos'] = ''\n",
    "    \n",
    "    # Process only rows where 'Dic (Cupo/Insc)' column is not empty\n",
    "    mask = df['Dic (Cupo/Insc)'].notna()\n",
    "    \n",
    "    \"\"\"\n",
    "    for idx in df[mask].index:\n",
    "        cupo_str = df.at[idx, 'Dic (Cupo/Insc)']\n",
    "        # Extract numbers between parentheses\n",
    "        if 'Nor (' in cupo_str:\n",
    "            numbers = cupo_str.split('(')[1].split(')')[0]\n",
    "            if '/' in numbers:\n",
    "                vacantes, inscritos = numbers.split('/')\n",
    "                df.at[idx, 'Vacantes'] = int(vacantes)\n",
    "                df.at[idx, 'Inscritos'] = int(inscritos) \"\"\"\n",
    "                \n",
    "    import re\n",
    "\n",
    "    for idx in df[mask].index:\n",
    "        cupo_str = df.at[idx, 'Dic (Cupo/Insc)']\n",
    "        # Use regex to extract numbers\n",
    "        match = re.search(r'\\((\\d+)/(\\d+)\\)', cupo_str)\n",
    "        if match:\n",
    "            vacantes = int(match.group(1))\n",
    "            inscritos = int(match.group(2))\n",
    "            df.at[idx, 'Vacantes'] = vacantes\n",
    "            df.at[idx, 'Inscritos'] = inscritos\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop('Dic (Cupo/Insc)', axis=1)\n",
    "    \n",
    "    # Reorder columns to put Vacantes and Inscritos after the second column\n",
    "    cols = df.columns.tolist()\n",
    "    vacantes_index = cols.index('Vacantes')\n",
    "    inscritos_index = cols.index('Inscritos')\n",
    "    cols = cols[:2] + ['Vacantes', 'Inscritos'] + [col for col in cols[2:] if col not in ['Vacantes', 'Inscritos']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN2_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = capacity_and_enrolled()\n",
    "    print(\"CSV file has been processed successfully.\")\n",
    "    # Print first few rows to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar NombreSala y Capacidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def separate_classroom_data():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN2_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns\n",
    "    df['Sala'] = ''\n",
    "    df['Capacidad'] = ''\n",
    "    \n",
    "    # Process only rows where 'Sala(Cap)' column is not empty\n",
    "    mask = df['Sala(Cap)'].notna()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        sala_str = df.at[idx, 'Sala(Cap)']\n",
    "        if '(' in sala_str and ')' in sala_str:\n",
    "            # Split into sala and capacidad\n",
    "            sala = sala_str.split('(')[0]\n",
    "            capacidad = sala_str.split('(')[1].split(')')[0]\n",
    "            df.at[idx, 'Sala'] = sala\n",
    "            if capacidad.isdigit():  # Only convert to int if it's a number\n",
    "                df.at[idx, 'Capacidad'] = int(capacidad)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop('Sala(Cap)', axis=1)\n",
    "    \n",
    "    # Reorder columns to put Sala and Capacidad where Sala(Cap) was\n",
    "    cols = df.columns.tolist()\n",
    "    cols = [col for col in cols if col not in ['Sala', 'Capacidad']]\n",
    "    sala_cap_position = 10  # La posición mencionada (11) menos 1 por el índice base 0\n",
    "    cols = cols[:sala_cap_position] + ['Sala', 'Capacidad'] + cols[sala_cap_position:]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN3_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = separate_classroom_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horas Impartidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def obtain_hours():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN3_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new column for hours\n",
    "    df['Cantidad Horas'] = 0\n",
    "    \n",
    "    # Process only rows where 'Detalle Hora' column is not empty\n",
    "    mask = df['Detalle Hora'].notna()\n",
    "    \n",
    "    def calculate_hours(time_range):\n",
    "        try:\n",
    "            # Split the time range and extract start and end times\n",
    "            start_str, end_str = time_range.split(' - ')\n",
    "            \n",
    "            # Convert times to datetime objects for easier calculation\n",
    "            start_time = datetime.strptime(start_str, '%H:%M')\n",
    "            end_time = datetime.strptime(end_str, '%H:%M')\n",
    "            \n",
    "            # Calculate the difference in hours\n",
    "            diff = end_time - start_time\n",
    "            hours = diff.seconds / 3600\n",
    "            \n",
    "            return int(hours)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    # Apply the calculation to each row with time data\n",
    "    df.loc[mask, 'Cantidad Horas'] = df.loc[mask, 'Detalle Hora'].apply(calculate_hours)\n",
    "    \n",
    "    # Reorder columns to put Cantidad Horas after Detalle Hora\n",
    "    cols = df.columns.tolist()\n",
    "    detalle_hora_index = cols.index('Detalle Hora')\n",
    "    cols = [col for col in cols if col != 'Cantidad Horas']\n",
    "    cols.insert(detalle_hora_index + 1, 'Cantidad Horas')\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN4_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = obtain_hours()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expandir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def expand_data():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN4_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # First group of columns to propagate until new Asig. Par. or end marker\n",
    "    course_columns = ['Asig. Par.', 'Asig. Nombre', 'Vacantes', 'Inscritos', 'Carreras']\n",
    "    \n",
    "    # Second group of columns to propagate until new activity/professor\n",
    "    activity_columns = ['Actividad', 'RUT', 'Profesor_Nombre']\n",
    "    \n",
    "    # Initialize variables for current values\n",
    "    current_course = {}\n",
    "    current_activity = {}\n",
    "    \n",
    "    # Process the DataFrame row by row\n",
    "    for i in range(len(df)):\n",
    "        # Check if we've reached the end marker\n",
    "        if df.at[i, 'Asig. Par.'] == 'Fin Guía Académica por Facultades':\n",
    "            break\n",
    "            \n",
    "        # Process course-level information\n",
    "        if pd.notna(df.at[i, 'Asig. Par.']):\n",
    "            # New course found, update current values\n",
    "            for col in course_columns:\n",
    "                current_course[col] = df.at[i, col]\n",
    "        else:\n",
    "            # Propagate course information\n",
    "            for col in course_columns:\n",
    "                df.at[i, col] = current_course.get(col, '')\n",
    "                \n",
    "        # Process activity-level information\n",
    "        has_new_activity = pd.notna(df.at[i, 'Actividad']) and df.at[i, 'Actividad'] != current_activity.get('Actividad', '')\n",
    "        has_new_professor = pd.notna(df.at[i, 'RUT']) and df.at[i, 'RUT'] != current_activity.get('RUT', '')\n",
    "        \n",
    "        if has_new_activity or has_new_professor:\n",
    "            # New activity or professor found, update current values\n",
    "            current_activity = {}\n",
    "            for col in activity_columns:\n",
    "                if pd.notna(df.at[i, col]):\n",
    "                    current_activity[col] = df.at[i, col]\n",
    "        else:\n",
    "            # Propagate activity information if we have stored values\n",
    "            for col in activity_columns:\n",
    "                if pd.isna(df.at[i, col]) and col in current_activity:\n",
    "                    df.at[i, col] = current_activity[col]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN5_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = expand_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar Dia/Bloques repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franco\\AppData\\Local\\Temp\\ipykernel_35640\\2063543194.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  processed_df = pd.concat([processed_df, block_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_schedule():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN5_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create a copy of the DataFrame to store the processed results\n",
    "    processed_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Get unique combinations of Asig. Par. and RUT to identify professor blocks\n",
    "    unique_blocks = df[df['RUT'].notna()][['Asig. Par.', 'Actividad', 'RUT']].drop_duplicates()\n",
    "    \n",
    "    # Process each block separately\n",
    "    for _, block in unique_blocks.iterrows():\n",
    "        # Get all rows for this professor and course\n",
    "        mask = (\n",
    "            (df['Asig. Par.'] == block['Asig. Par.']) & \n",
    "            (df['Actividad'] == block['Actividad']) & \n",
    "            (df['RUT'] == block['RUT'])\n",
    "        )\n",
    "        block_data = df[mask].copy()\n",
    "        \n",
    "        # Drop duplicates based on Dia and Detalle Hora within this block\n",
    "        if not block_data.empty:\n",
    "            # Keep only first occurrence of each Dia + Detalle Hora combination\n",
    "            block_data = block_data.drop_duplicates(\n",
    "                subset=['Dia', 'Detalle Hora'],\n",
    "                keep='first'\n",
    "            )\n",
    "            \n",
    "            # Append the processed block to the result\n",
    "            processed_df = pd.concat([processed_df, block_data], ignore_index=True)\n",
    "    \n",
    "    # Add any remaining rows that weren't part of any professor block\n",
    "    mask_no_prof = df['RUT'].isna()\n",
    "    remaining_rows = df[mask_no_prof]\n",
    "    processed_df = pd.concat([processed_df, remaining_rows], ignore_index=True)\n",
    "    \n",
    "    # Sort to maintain original order as much as possible\n",
    "    if 'index' in processed_df.columns:\n",
    "        processed_df = processed_df.sort_values('index').drop('index', axis=1)\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    processed_df.to_csv('outputN6_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = process_schedule()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_duplicate_rows():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Columns to drop\n",
    "    drop_columns = ['Dia', 'Blo', 'Detalle Hora']\n",
    "    df = df.drop(columns=drop_columns, errors='ignore')\n",
    "    \n",
    "    # Group by all the specified columns, this will only merge rows that are\n",
    "    # identical in all these columns\n",
    "    check_columns = [\n",
    "        'Asig. Par.', 'Asig. Nombre', 'Vacantes', 'Inscritos', \n",
    "        'Carreras', 'Actividad', 'RUT', 'Profesor_Nombre'\n",
    "    ]\n",
    "    \n",
    "    # Group and sum hours\n",
    "    result_df = df.groupby(\n",
    "        by=check_columns,\n",
    "        dropna=False\n",
    "    ).agg({\n",
    "        'Sala': 'first',\n",
    "        'Capacidad': 'first',\n",
    "        'Cantidad Horas': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    result_df.to_csv('outputN7_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = merge_duplicate_rows()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chek Asigments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique courses: 295\n",
      "Unique courses: 295\n",
      "Unique Asignaturas: 175\n",
      "Unique Asignaturas: 175\n"
     ]
    }
   ],
   "source": [
    "#Cuantas \"Asig. Par.\" Unicas hay en outputN7_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN7_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Par.'].nunique()\n",
    "print(f\"Unique courses: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Par.\" Unicas hay en outputN6_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Par.'].nunique()\n",
    "print(f\"Unique courses: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Nombre\" Unicas hay en outputN7_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN7_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Nombre'].nunique()\n",
    "print(f\"Unique Asignaturas: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Nombre\" Unicas hay en outputN6_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Nombre'].nunique()\n",
    "print(f\"Unique Asignaturas: {unique_courses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#TODO: Revisar el campus\n",
    "def extract_level_from_code(code):\n",
    "    \"\"\"Extract the level number from the course code.\"\"\"\n",
    "    if pd.isna(code) or code == '':\n",
    "        return 1\n",
    "        \n",
    "    # Use regex to find the first number after letters\n",
    "    match = re.search(r'[A-Z]+(\\d)', code)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 1  # Default value if no number is found\n",
    "\n",
    "def convert_csv_to_json(input_csv='outputN7_Guia_FIA.csv', output_json=\"../agent_input/inputOfProfesores.json\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv, delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Sort the DataFrame by RUT and Carreras\n",
    "    df = df.sort_values(['RUT', 'Carreras'])\n",
    "    \n",
    "    # Initialize the result list and turno counter\n",
    "    profesores = []\n",
    "    turno_counter = 1\n",
    "    current_rut = None\n",
    "    \n",
    "    # Group by RUT to process each professor's courses\n",
    "    for rut, group in df.groupby('RUT'):\n",
    "        # Skip empty RUTs\n",
    "        if pd.isna(rut) or rut == '':\n",
    "            continue\n",
    "            \n",
    "        # Get the first occurrence of professor data\n",
    "        first_row = group.iloc[0]\n",
    "        \n",
    "        # Create profesor object\n",
    "        profesor = {\n",
    "            \"RUT\": rut,\n",
    "            \"Nombre\": first_row['Profesor_Nombre'],\n",
    "            \"Turno\": turno_counter,\n",
    "            \"Asignaturas\": []\n",
    "        }\n",
    "        \n",
    "        # Process each course for the professor\n",
    "        for _, row in group.iterrows():\n",
    "            asignatura = {\n",
    "                \"CodigoAsignatura\": row['Asig. Par.'],\n",
    "                \"Nombre\": row['Asig. Nombre'],\n",
    "                \"Nivel\": extract_level_from_code(row['Asig. Par.']),  # Extract level from code\n",
    "                \"Paralelo\": \"A\",  # Static value\n",
    "                \"Horas\": int(row['Cantidad Horas']),\n",
    "                \"Vacantes\": int(row['Vacantes']) if pd.notna(row['Vacantes']) else 0,\n",
    "                \"Campus\": \"Playa Brava\"  # Static value\n",
    "            }\n",
    "            profesor[\"Asignaturas\"].append(asignatura)\n",
    "        \n",
    "        profesores.append(profesor)\n",
    "        turno_counter += 1\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(profesores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return profesores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = convert_csv_to_json()\n",
    "        print(\"Conversion completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#TODO: Revisar el campus\n",
    "def convert_classrooms_to_json(input_csv='outputN7_Guia_FIA.csv', output_json=\"../agent_input/inputOfSala.json\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv, delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Get unique classrooms and their capacities\n",
    "    # Filter out empty values and 'VIRTU'\n",
    "    classrooms_df = df[['Sala', 'Capacidad']].copy()\n",
    "    classrooms_df = classrooms_df[\n",
    "        classrooms_df['Sala'].notna() & \n",
    "        (classrooms_df['Sala'] != '') & \n",
    "        (classrooms_df['Sala'] != 'VIRTU')\n",
    "    ].drop_duplicates()\n",
    "    \n",
    "    # Sort by classroom code\n",
    "    classrooms_df = classrooms_df.sort_values('Sala')\n",
    "    \n",
    "    # Create the list of classrooms with sequential turno\n",
    "    classrooms = []\n",
    "    turno_counter = 1\n",
    "\n",
    "    salas_kaufmann = {\n",
    "        \"KAUL1\": True,\n",
    "        \"KAUS1\": True,\n",
    "        \"KAUS2\": True,\n",
    "        \"KAUS3\": True,\n",
    "        \"KAUS4\": True,\n",
    "        \"KAUS5\": True,\n",
    "        \"KAUT1\": True,\n",
    "        \"KAUT2\": True,\n",
    "        \"KAUT3\": True,\n",
    "        \"KAUT4\": True,\n",
    "        \"KAUT5\": True,\n",
    "        \"KAUT6\": True,\n",
    "        \"KAUT7\": True,\n",
    "        \"KAUT8\": True,\n",
    "        \"KAU01\": True\n",
    "    }\n",
    "    \n",
    "    for _, row in classrooms_df.iterrows():\n",
    "\n",
    "        # Determine campus based on classroom code\n",
    "        campus = \"Kaufmann\" if row['Sala'] in salas_kaufmann else \"Playa Brava\"\n",
    "\n",
    "        classroom = {\n",
    "            \"Turno\": turno_counter,\n",
    "            \"Codigo\": row['Sala'],\n",
    "            \"Capacidad\": int(row['Capacidad']) if pd.notna(row['Capacidad']) else 0,\n",
    "            \"Campus\": campus\n",
    "        }\n",
    "        classrooms.append(classroom)\n",
    "        turno_counter += 1\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(classrooms, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return classrooms\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = convert_classrooms_to_json()\n",
    "        print(\"Conversion completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad total de asignaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputOfProfesores.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_subjects)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Use the function\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcount_unique_subjects\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputOfProfesores.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero de asignaturas diferentes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36mcount_unique_subjects\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount_unique_subjects\u001b[39m(filename):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Read JSON file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Create a set to store unique subject codes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Franco\\Documents\\GitHub\\Work\\Francoo86\\Implementaciones-MAS\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputOfProfesores.json'"
     ]
    }
   ],
   "source": [
    "def count_unique_subjects(filename):\n",
    "    # Read JSON file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a set to store unique subject codes\n",
    "    unique_subjects = set()\n",
    "    \n",
    "    # Iterate through each professor and their subjects\n",
    "    for profesor in data:\n",
    "        for asignatura in profesor['Asignaturas']:\n",
    "            unique_subjects.add(asignatura['CodigoAsignatura'])\n",
    "    \n",
    "    return len(unique_subjects)\n",
    "\n",
    "# Use the function\n",
    "result = count_unique_subjects('agete.json')\n",
    "print(f\"Número de asignaturas diferentes: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Campus de cada Sala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 45\n",
      "Salas no encontradas: 28\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_brava = {\n",
    "    \"AOS01\": True,\n",
    "    \"B1\": True,\n",
    "    \"B3\": True,\n",
    "    \"B4\": True,\n",
    "    \"B5\": True,\n",
    "    \"B6\": True,\n",
    "    \"B7\": True,\n",
    "    \"B8\": True,\n",
    "    \"CM3\": True,\n",
    "    \"CM4\": True,\n",
    "    \"CM5\": True,\n",
    "    \"CM6\": True,\n",
    "    \"CM8\": True,\n",
    "    \"CPAS\": True,\n",
    "    \"CRIQ\": True,\n",
    "    \"CRP1\": True,\n",
    "    \"CRP2\": True,\n",
    "    \"CRP3\": True,\n",
    "    \"CRP31\": True,\n",
    "    \"CRP32\": True,\n",
    "    \"CRP33\": True,\n",
    "    \"CRP34\": True,\n",
    "    \"CRP35\": True,\n",
    "    \"CRP4\": True,\n",
    "    \"CRP41\": True,\n",
    "    \"CRP42\": True,\n",
    "    \"CRP43\": True,\n",
    "    \"CRP44\": True,\n",
    "    \"CRP46\": True,\n",
    "    \"CSER\": True,\n",
    "    \"C1\": True,\n",
    "    \"C2\": True,\n",
    "    \"C3\": True,\n",
    "    \"C4\": True,\n",
    "    \"C5\": True,\n",
    "    \"C6\": True,\n",
    "    \"EJEMP\": True,\n",
    "    \"ELCT\": True,\n",
    "    \"EST01\": True,\n",
    "    \"E1\": True,\n",
    "    \"E10\": True,\n",
    "    \"E11\": True,\n",
    "    \"E2\": True,\n",
    "    \"E3\": True,\n",
    "    \"E4\": True,\n",
    "    \"E5\": True,\n",
    "    \"E6\": True,\n",
    "    \"E7\": True,\n",
    "    \"FAESC\": True,\n",
    "    \"F1\": True,\n",
    "    \"F6\": True,\n",
    "    \"F7\": True,\n",
    "    \"GCOV\": True,\n",
    "    \"GESM\": True,\n",
    "    \"GIM\": True,\n",
    "    \"GIMC\": True,\n",
    "    \"GIMN\": True,\n",
    "    \"G21M\": True,\n",
    "    \"IC1\": True,\n",
    "    \"IC2\": True,\n",
    "    \"IC3\": True,\n",
    "    \"IC4\": True,\n",
    "    \"IC8\": True,\n",
    "    \"IES\": True,\n",
    "    \"IM2\": True,\n",
    "    \"IM3\": True,\n",
    "    \"IM4\": True,\n",
    "    \"INC1\": True,\n",
    "    \"IOLAB\": True,\n",
    "    \"J1\": True,\n",
    "    \"K1\": True,\n",
    "    \"K2\": True,\n",
    "    \"K3\": True,\n",
    "    \"K4\": True,\n",
    "    \"K5\": True,\n",
    "    \"K6\": True,\n",
    "    \"K7\": True,\n",
    "    \"LADU\": True,\n",
    "    \"LAMB\": True,\n",
    "    \"LANAQ\": True,\n",
    "    \"LANT\": True,\n",
    "    \"LATAS\": True,\n",
    "    \"LAUT\": True,\n",
    "    \"LBIG\": True,\n",
    "    \"LBIO\": True,\n",
    "    \"LCOA\": True,\n",
    "    \"LCOI\": True,\n",
    "    \"LCOM\": True,\n",
    "    \"LCSI\": True,\n",
    "    \"LC1\": True,\n",
    "    \"LC2\": True,\n",
    "    \"LC3\": True,\n",
    "    \"LC4\": True,\n",
    "    \"LC6\": True,\n",
    "    \"LC7\": True,\n",
    "    \"LC8\": True,\n",
    "    \"LEDI1\": True,\n",
    "    \"LEDI2\": True,\n",
    "    \"LELE\": True,\n",
    "    \"LELM\": True,\n",
    "    \"LFIS\": True,\n",
    "    \"LFLO\": True,\n",
    "    \"LHID\": True,\n",
    "    \"LINF\": True,\n",
    "    \"LITI\": True,\n",
    "    \"LMIN\": True,\n",
    "    \"LM1\": True,\n",
    "    \"LM2\": True,\n",
    "    \"LOPU\": True,\n",
    "    \"LP\": True,\n",
    "    \"LPIR\": True,\n",
    "    \"LPME\": True,\n",
    "    \"LPOT\": True,\n",
    "    \"LQ1\": True,\n",
    "    \"LQ2\": True,\n",
    "    \"LQ3\": True,\n",
    "    \"LQ4\": True,\n",
    "    \"LROB\": True,\n",
    "    \"LRSM\": True,\n",
    "    \"LSD\": True,\n",
    "    \"LSO\": True,\n",
    "    \"LSS\": True,\n",
    "    \"MMIN\": True,\n",
    "    \"MULT\": True,\n",
    "    \"ODLSC\": True,\n",
    "    \"ODPCM\": True,\n",
    "    \"ODPOA\": True,\n",
    "    \"ODPOB\": True,\n",
    "    \"ODUIM\": True,\n",
    "    \"ONLI\": True,\n",
    "    \"ONLIN\": True,\n",
    "    \"PLC\": True,\n",
    "    \"PRESE\": True,\n",
    "    \"SCAL\": True,\n",
    "    \"SFE20\": True,\n",
    "    \"SFH30\": True,\n",
    "    \"SIMA\": True,\n",
    "    \"SIMI\": True,\n",
    "    \"SP\": True,\n",
    "    \"SRA\": True,\n",
    "    \"SRA01\": True,\n",
    "    \"SRCS\": True,\n",
    "    \"SRE\": True,\n",
    "    \"SREC\": True,\n",
    "    \"SRED\": True,\n",
    "    \"SRFM\": True,\n",
    "    \"SRI\": True,\n",
    "    \"SRQ\": True,\n",
    "    \"TBD\": True,\n",
    "    \"TER\": True,\n",
    "    \"TRT\": True,\n",
    "    \"TSIM\": True,\n",
    "    \"VIDQ\": True,\n",
    "    \"VIRTU\": True\n",
    "}\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_brava:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 13\n",
      "Salas no encontradas: 60\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_kaufmann = {\n",
    "    \"KAUL1\": True,\n",
    "    \"KAUS1\": True,\n",
    "    \"KAUS2\": True,\n",
    "    \"KAUS3\": True,\n",
    "    \"KAUS4\": True,\n",
    "    \"KAUS5\": True,\n",
    "    \"KAUT1\": True,\n",
    "    \"KAUT2\": True,\n",
    "    \"KAUT3\": True,\n",
    "    \"KAUT4\": True,\n",
    "    \"KAUT5\": True,\n",
    "    \"KAUT6\": True,\n",
    "    \"KAUT7\": True,\n",
    "    \"KAUT8\": True,\n",
    "    \"KAU01\": True\n",
    "}\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_kaufmann:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 0\n",
      "Salas no encontradas: 73\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_huayquique = {\n",
    "    \"HLA1\": True, \"HLA2\": True, \"HLA3\": True, \"H10\": True, \"H12\": True,\n",
    "    \"H13\": True, \"H14\": True, \"H16\": True, \"H17\": True, \"H18\": True,\n",
    "    \"H20\": True, \"H21\": True, \"H22\": True, \"H4\": True, \"H5\": True,\n",
    "    \"H6\": True, \"LBDE\": True, \"LCHUA\": True, \"LCUL\": True, \"LECO\": True,\n",
    "    \"LFVE\": True, \"LGEN\": True, \"LINV\": True, \"LMIC\": True, \"LOCE\": True,\n",
    "    \"LPEC\": True, \"LZOO\": True, \"SRAH\": True\n",
    "}\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_huayquique:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 0\n",
      "Salas no encontradas: 73\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_lynch = {\n",
    "    \"BIOM\": True,\n",
    "    \"EK01\": True,\n",
    "    \"EK02\": True,\n",
    "    \"EK03\": True,\n",
    "    \"IRER\": True,\n",
    "    \"LBI2\": True,\n",
    "    \"LFEJ\": True,\n",
    "    \"LFST\": True,\n",
    "    \"LGIT\": True\n",
    "}\n",
    "\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_lynch:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 0\n",
      "Salas no encontradas: 73\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_canchones = {\n",
    "    \"CANC\": True,\n",
    "    \"CANH\": True,\n",
    "    \"CAN1\": True,\n",
    "    \"CAN2\": True,\n",
    "    \"CAN3\": True,\n",
    "    \"CAN4\": True,\n",
    "    \"CAN5\": True,\n",
    "    \"TERR\": True\n",
    "}\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_canchones:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salas encontradas: 0\n",
      "Salas no encontradas: 73\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de salas en Huayquique\n",
    "salas_astoreca = {\n",
    "    \"ASTO1\": True,\n",
    "    \"ASTO2\": True,\n",
    "    \"ASTO3\": True,\n",
    "    \"ASTO4\": True,\n",
    "    \"ASTO5\": True\n",
    "}\n",
    "\n",
    "# Cargar archivo JSON\n",
    "with open(\"../agent_input/inputOfSala.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Inicializar contadores\n",
    "salas_encontradas = 0\n",
    "salas_no_encontradas = 0\n",
    "\n",
    "# Contar matches y no-matches\n",
    "for sala in data:\n",
    "    codigo = sala.get(\"Codigo\", \"\")\n",
    "    if codigo in salas_astoreca:\n",
    "        salas_encontradas += 1\n",
    "    else:\n",
    "        salas_no_encontradas += 1\n",
    "\n",
    "print(f\"Salas encontradas: {salas_encontradas}\")\n",
    "print(f\"Salas no encontradas: {salas_no_encontradas}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
